{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a786c77c",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* RunnableWithMessageHistory\n",
    "  * Analogous to using ConversationChain (deprecated) with the default ConversationBufferMemory\n",
    "  * Make llm can answer question based on memory\n",
    "* Simulate ConversationBufferWindowMemory to be used by RunnableWithMessageHistory\n",
    "  * The first solution: Keep only last k chats in memory\n",
    "  * The second solution: Just pass the last k chats from the memory to the llm, the memory will not be affected, as previous data not affected and keep adding new data to it.\n",
    "* ConversationTokenBufferMemory\n",
    "  * Not consider integrate it with RunnableWithMessageHistory until we have good usecase or example of using it\n",
    "* Simulate ConversationSummaryMemory to be used by RunnableWithMessageHistory\n",
    "\n",
    "## Reference\n",
    "* [How to add memory to chatbots](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dcd5",
   "metadata": {},
   "source": [
    "## RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f518f5",
   "metadata": {
    "height": 147,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301270b-7a69-40f8-9934-f840499b6ae9",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0924a2-ce2d-44ba-a806-c1195720c237",
   "metadata": {
    "height": 249
   },
   "outputs": [],
   "source": [
    "llm_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ad6fe2",
   "metadata": {
    "height": 96,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in /opt/homebrew/anaconda3/lib/python3.11/site-packages (0.1.19)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.24 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.2.24)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.32.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain-openai) (1.37.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.24->langchain-openai) (6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.24->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.24->langchain-openai) (0.1.93)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.24->langchain-openai) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.24->langchain-openai) (1.10.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.24->langchain-openai) (8.2.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.7.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.24->langchain-openai) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.24->langchain-openai) (3.10.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (1.26.16)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in /opt/homebrew/anaconda3/lib/python3.11/site-packages (0.2.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.23 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain) (0.2.24)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain) (0.1.93)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.23->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.23->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain) (2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-openai\n",
    "%pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88bdf13d",
   "metadata": {
    "height": 147,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in memory:\n",
    "        memory[session_id] = InMemoryChatMessageHistory()\n",
    "    return memory[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(llm, get_session_history)\n",
    "config = {\"configurable\": {\"session_id\": \"InMemoryChatMessageHistory\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db24677d",
   "metadata": {
    "height": 45,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Joseph! How can I assist you today?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, my name is Joseph\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3ef937",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is 1+1?\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf3339a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Joseph.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is ny name?\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5018cb0a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Joseph\n",
      "AI: Hi Joseph! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2.\n",
      "Human: What is ny name?\n",
      "AI: Your name is Joseph.\n"
     ]
    }
   ],
   "source": [
    "print(memory.get(config.get(\"configurable\").get(\"session_id\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98e9ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f9c9c",
   "metadata": {},
   "source": [
    "Solution 1: Simulating ConversionBufferWindowMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea6233e",
   "metadata": {
    "height": 45,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import List\n",
    "\n",
    "class ConversationBufferWindowMemory(BaseChatMessageHistory):\n",
    "    \"\"\"A chat message history that only keeps the last K messages.\"\"\"\n",
    "    \n",
    "    def __init__(self, buffer_size: int):\n",
    "        super().__init__()\n",
    "        self.buffer_size = buffer_size\n",
    "        self.messages: List[BaseMessage] = []\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        \"\"\"Add a list of messages to the store, keeping only the last K.\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        # Ensure only the last K messages are kept\n",
    "        self.messages = self.messages[-self.buffer_size:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the chat message history.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "# Example usage with RunnableWithMessageHistory\n",
    "def get_conversation_buffer(buffer_size):\n",
    "    def internal(session_id):\n",
    "    # Assuming each session has its own buffer size requirement, \n",
    "    # you could customize this. For simplicity, we'll use a fixed size.\n",
    "      return ConversationBufferWindowMemory(buffer_size=buffer_size)\n",
    "    return internal\n",
    "\n",
    "# Assuming you have a Runnable `chain` already defined\n",
    "chain_with_limited_history = RunnableWithMessageHistory(\n",
    "    llm,\n",
    "    get_session_history=get_conversation_buffer(1),\n",
    ")\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4faaa952",
   "metadata": {
    "height": 45,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Joseph! How can I assist you today?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain_with_limited_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, my name is Joseph\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb20ddaa",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain_with_limited_history.invoke(\n",
    "    [HumanMessage(content=\"What is 1+1?\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "489b2194",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. If you'd like to tell me your name, feel free!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain_with_limited_history.invoke(\n",
    "    [HumanMessage(content=\"What is ny name?\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e182176",
   "metadata": {},
   "source": [
    "Solution 2: Use trimming history data provided by official website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35b5c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hey there! I'm Joseph.\"),\n",
       " AIMessage(content='Hello!'),\n",
       " HumanMessage(content='How are you today?'),\n",
       " AIMessage(content='Fine thanks!'),\n",
       " HumanMessage(content='My mom is Maggie mee.'),\n",
       " AIMessage(content='Oh i see!')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"Hey there! I'm Joseph.\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
    "demo_ephemeral_chat_history.add_user_message(\"How are you today?\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Fine thanks!\")\n",
    "demo_ephemeral_chat_history.add_user_message(\"My mom is Maggie mee.\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Oh i see!\")\n",
    "\n",
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ed4b4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Joseph.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 81, 'total_tokens': 86}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ba606877f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a212343-4b4e-472a-8104-f4740fbe05bc-0', usage_metadata={'input_tokens': 81, 'output_tokens': 5, 'total_tokens': 86})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chain = prompt | chat\n",
    "\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_ephemeral_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "chain_with_message_history.invoke(\n",
    "    {\"input\": \"What's my name?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cc08cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.messages import trim_messages\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# The demo_ephemeral_chat_history will not be affected by the trimming\n",
    "# This just get the last 2 messages from the demo_ephemeral_chat_history\n",
    "trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n",
    "\n",
    "chain_with_trimming = (\n",
    "    RunnablePassthrough.assign(chat_history=itemgetter(\"chat_history\") | trimmer)\n",
    "    | prompt\n",
    "    | chat\n",
    ")\n",
    "\n",
    "chain_with_trimmed_history = RunnableWithMessageHistory(\n",
    "    chain_with_trimming,\n",
    "    lambda session_id: demo_ephemeral_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85c71e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P. Sherman lives at 42 Wallaby Way, Sydney. This address is a reference from the animated movie \"Finding Nemo.\"'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain_with_trimmed_history.invoke(\n",
    "    {\"input\": \"Where does P. Sherman live?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f82d5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hey there! I'm Joseph.\"),\n",
       " AIMessage(content='Hello!'),\n",
       " HumanMessage(content='How are you today?'),\n",
       " AIMessage(content='Fine thanks!'),\n",
       " HumanMessage(content='My mom is Maggie mee.'),\n",
       " AIMessage(content='Oh i see!'),\n",
       " HumanMessage(content=\"What's my name?\"),\n",
       " AIMessage(content='Your name is Joseph.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 81, 'total_tokens': 86}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ba606877f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a212343-4b4e-472a-8104-f4740fbe05bc-0', usage_metadata={'input_tokens': 81, 'output_tokens': 5, 'total_tokens': 86}),\n",
       " HumanMessage(content='Where does P. Sherman live?'),\n",
       " AIMessage(content='P. Sherman lives at 42 Wallaby Way, Sydney. This is a reference from the animated movie \"Finding Nemo.\"', response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 101, 'total_tokens': 126}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ba606877f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-c300669b-cbad-4504-a62a-14767911d8ac-0', usage_metadata={'input_tokens': 101, 'output_tokens': 25, 'total_tokens': 126}),\n",
       " HumanMessage(content='What is my mother name?'),\n",
       " AIMessage(content=\"Your mother's name is Maggie Mee.\", response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 140, 'total_tokens': 147}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_611b667b19', 'finish_reason': 'stop', 'logprobs': None}, id='run-6a63f578-2dca-40eb-859e-d313a979388c-0', usage_metadata={'input_tokens': 140, 'output_tokens': 7, 'total_tokens': 147}),\n",
       " HumanMessage(content='Where does P. Sherman live?'),\n",
       " AIMessage(content='P. Sherman lives at 42 Wallaby Way, Sydney. This address is a reference from the animated movie \"Finding Nemo.\"', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 55, 'total_tokens': 81}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ba606877f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f2fad06d-bd5d-415b-a1db-6c4a80f1467d-0', usage_metadata={'input_tokens': 55, 'output_tokens': 26, 'total_tokens': 81})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba2eeb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals, including your mother's name. If you have any other questions or need assistance with something else, feel free to ask!\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain_with_trimmed_history.invoke(\n",
    "    {\"input\": \"What is my mother name?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d67ce29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hey there! I'm Joseph.\"),\n",
       " AIMessage(content='Hello!'),\n",
       " HumanMessage(content='How are you today?'),\n",
       " AIMessage(content='Fine thanks!'),\n",
       " HumanMessage(content='My mom is Maggie mee.'),\n",
       " AIMessage(content='Oh i see!'),\n",
       " HumanMessage(content=\"What's my name?\"),\n",
       " AIMessage(content='Your name is Joseph.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 81, 'total_tokens': 86}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ba606877f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a212343-4b4e-472a-8104-f4740fbe05bc-0', usage_metadata={'input_tokens': 81, 'output_tokens': 5, 'total_tokens': 86}),\n",
       " HumanMessage(content='Where does P. Sherman live?'),\n",
       " AIMessage(content='P. Sherman lives at 42 Wallaby Way, Sydney. This is a reference from the animated movie \"Finding Nemo.\"', response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 101, 'total_tokens': 126}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ba606877f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-c300669b-cbad-4504-a62a-14767911d8ac-0', usage_metadata={'input_tokens': 101, 'output_tokens': 25, 'total_tokens': 126}),\n",
       " HumanMessage(content='What is my mother name?'),\n",
       " AIMessage(content=\"Your mother's name is Maggie Mee.\", response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 140, 'total_tokens': 147}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_611b667b19', 'finish_reason': 'stop', 'logprobs': None}, id='run-6a63f578-2dca-40eb-859e-d313a979388c-0', usage_metadata={'input_tokens': 140, 'output_tokens': 7, 'total_tokens': 147}),\n",
       " HumanMessage(content='Where does P. Sherman live?'),\n",
       " AIMessage(content='P. Sherman lives at 42 Wallaby Way, Sydney. This address is a reference from the animated movie \"Finding Nemo.\"', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 55, 'total_tokens': 81}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ba606877f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f2fad06d-bd5d-415b-a1db-6c4a80f1467d-0', usage_metadata={'input_tokens': 55, 'output_tokens': 26, 'total_tokens': 81}),\n",
       " HumanMessage(content='What is my mother name?'),\n",
       " AIMessage(content=\"I'm sorry, but I don't have access to personal information about individuals, including your mother's name. If you have any other questions or need assistance with something else, feel free to ask!\", response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 74, 'total_tokens': 111}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ba606877f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-d00be6e6-0856-4248-9401-ffa1d3651384-0', usage_metadata={'input_tokens': 74, 'output_tokens': 37, 'total_tokens': 111})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2931b92",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb9020ed",
   "metadata": {
    "height": 79,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43582ee6",
   "metadata": {
    "height": 147,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "284288e1",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Charming!'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff55d5d",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af48616a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hey there! I'm Joseph.\"),\n",
       " AIMessage(content='Hello!'),\n",
       " HumanMessage(content='How are you today?'),\n",
       " AIMessage(content='Fine thanks!'),\n",
       " HumanMessage(content='My mom is Maggie mee.'),\n",
       " AIMessage(content='Oh i see!')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"Hey there! I'm Joseph.\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
    "demo_ephemeral_chat_history.add_user_message(\"How are you today?\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Fine thanks!\")\n",
    "demo_ephemeral_chat_history.add_user_message(\"My mom is Maggie mee.\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Oh i see!\")\n",
    "\n",
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f715ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_ephemeral_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90d1b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def summarize_messages(chain_input):\n",
    "    stored_messages = demo_ephemeral_chat_history.messages\n",
    "    if len(stored_messages) == 0:\n",
    "        return False\n",
    "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"placeholder\", \"{chat_history}\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    summarization_chain = summarization_prompt | chat\n",
    "\n",
    "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
    "\n",
    "    demo_ephemeral_chat_history.clear()\n",
    "\n",
    "    demo_ephemeral_chat_history.add_message(summary_message)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "chain_with_summarization = (\n",
    "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
    "    | chain_with_message_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb74859f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You mentioned that your name is Joseph.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain_with_summarization.invoke(\n",
    "    {\"input\": \"What did I say my name was?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ba827aa",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Joseph introduced himself and mentioned that his mom is named Maggie Mee. He also asked how I was doing, to which I responded that I was fine.\n",
      "Human: What did I say my name was?\n",
      "AI: You mentioned that your name is Joseph.\n"
     ]
    }
   ],
   "source": [
    "print(demo_ephemeral_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c06f617b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your mother's name is Maggie Mee.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain_with_summarization.invoke(\n",
    "    {\"input\": \"What is my mother name?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c07922b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Joseph introduced himself and shared that his mom's name is Maggie Mee. He also inquired about my well-being, to which I responded that I was fine.\n",
      "Human: What is my mother name?\n",
      "AI: Your mother's name is Maggie Mee.\n"
     ]
    }
   ],
   "source": [
    "print(demo_ephemeral_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52696c8c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48690d13",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bba1f8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a62e89",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953ad8d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb2617",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1989bf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8dca6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9120949",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55849fb5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae491b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3aec3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae51a3f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06894001",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb674c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158f4c8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebe066",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65caee4d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04799230",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30795c10",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed59c4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb43eee3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
